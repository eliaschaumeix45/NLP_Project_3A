{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7vLAznfdpbW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiwZwHoUdswN"
      },
      "source": [
        "# 1) librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKvDDu3fdvah",
        "outputId": "7d5e7cc0-59cf-4d5e-b76a-79144c392c3d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import  TweetTokenizer, sent_tokenize\n",
        "# Libraries for text preprocessing\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "import nltk.data\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from collections import OrderedDict\n",
        "#!pip install wordcloud\n",
        "\n",
        "#Word cloud\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "\n",
        "from scipy import sparse\n",
        "#Function for sorting tf_idf in descending order\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "!pip install rake_nltk\n",
        "!pip install yake\n",
        "from rake_nltk import Rake\n",
        "import yake\n",
        "from gensim.summarization import keywords\n",
        "import spacy\n",
        "import operator\n",
        "from rake_nltk import Rake\n",
        "from sklearn.decomposition import PCA\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting rake_nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rake_nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->rake_nltk) (1.15.0)\n",
            "Building wheels for collected packages: rake-nltk\n",
            "  Building wheel for rake-nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7819 sha256=b26fbc6839798a33bae44ad7c0a48b1fc9a83ef2044d40b075c061bc875f3717\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
            "Successfully built rake-nltk\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.4\n",
            "Collecting yake\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/7e/85ef0ce2e6a6756cc27ea7afa13a25d112d583c8b0a6639dd33e5249adc3/yake-0.4.7-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake) (1.19.5)\n",
            "Collecting segtok\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Collecting jellyfish\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/a6/4d039bc827a102f62ce7a7910713e38fdfd7c7a40aa39c72fb14938a1473/jellyfish-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake) (0.8.9)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake) (7.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake) (2.5.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake) (2019.12.20)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->yake) (4.4.2)\n",
            "Building wheels for collected packages: segtok\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=cc0a6656dd2ee63e8adcdf8ed46360cecedc8196b64db3e080aaddc9a6a8e96a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "Successfully built segtok\n",
            "Installing collected packages: segtok, jellyfish, yake\n",
            "Successfully installed jellyfish-0.8.2 segtok-1.5.10 yake-0.4.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au5Ls1PId51w"
      },
      "source": [
        "# 2) Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVuJdnczd-DC",
        "outputId": "b7ab5d7c-9a1d-48f1-cd7c-07735681b101"
      },
      "source": [
        "#df = pd.read_csv(\"/Users/eliaschaumeix/Desktop/3A/S2/NLP/df.csv\")\n",
        "df = pd.read_csv(\"df.csv\")\n",
        "\n",
        "acta_literaria = df.loc[:,['abstract_en','keywords_en']].dropna()\n",
        "acta_literaria.info()\n",
        "acta_literaria = acta_literaria.reset_index()\n",
        "\n",
        "n = acta_literaria.shape[0]\n",
        "\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "stem = PorterStemmer()\n",
        "\n",
        "##Creating a list of stop words and adding custom stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "##Creating a list of custom stopwords\n",
        "new_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\", \"abstract\"]\n",
        "stop_words = stop_words.union(new_words)\n",
        "\n",
        "acta_literaria['abstract_sentences'] = \"\"\n",
        "tokenizer_words = TweetTokenizer()\n",
        "for i in range (len(acta_literaria)):\n",
        "    tokens_sentences = [tokenizer_words.tokenize(t) for t in nltk.sent_tokenize(acta_literaria['abstract_en'][i])]\n",
        "    acta_literaria['abstract_sentences'][i] = tokens_sentences\n",
        "\n",
        "x = acta_literaria['abstract_sentences'].copy()\n",
        "for i in range(acta_literaria.shape[0]) : \n",
        "    sentences = x[i]\n",
        "    new = []\n",
        "    for sentence in sentences :\n",
        "        sentence_str = ' '.join(sentence)\n",
        "        new.append(sentence_str)\n",
        "    x[i] = new\n",
        "acta_literaria['sentence'] = \"\"\n",
        "acta_literaria['sentence'] = x\n",
        "\n",
        "corpus = []\n",
        "for i in range(n):\n",
        "    #Remove punctuations\n",
        "    text = re.sub('[^a-zA-Z]', ' ', acta_literaria['abstract_en'][i])\n",
        "    \n",
        "    #Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    ##Convert to list from string\n",
        "    text = text.split()\n",
        "    \n",
        "    ##Stemming\n",
        "    ps=PorterStemmer()\n",
        "    #Lemmatisation\n",
        "    lem = WordNetLemmatizer()\n",
        "    text = [lem.lemmatize(word) for word in text if not word in  \n",
        "            stop_words]\n",
        "    text = [lem.lemmatize(word,'v') for word in text if not word in  \n",
        "            stop_words]\n",
        "    text = \" \".join(text)\n",
        "    corpus.append(text)\n",
        "\n",
        "corpus1 = []\n",
        "for i in range(0, acta_literaria.shape[0]):\n",
        "    res  = []\n",
        "    for element in acta_literaria['sentence'][i] :\n",
        "        #Remove punctuations\n",
        "        text = re.sub('[^a-zA-Z]', ' ', element)\n",
        "        \n",
        "        #Convert to lowercase\n",
        "        text = text.lower()\n",
        "        \n",
        "        #remove tags\n",
        "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "        \n",
        "        # remove special characters and digits\n",
        "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "        \n",
        "        ##Convert to list from string\n",
        "        text = text.split()\n",
        "        \n",
        "        ##Stemming\n",
        "        ps=PorterStemmer()\n",
        "        #Lemmatisation\n",
        "        lem = WordNetLemmatizer()\n",
        "        text = [lem.lemmatize(word) for word in text if not word in  \n",
        "                stop_words]\n",
        "        text = [lem.lemmatize(word,'v') for word in text if not word in  \n",
        "                stop_words]\n",
        "        text = \" \".join(text)\n",
        "        res.append(text)\n",
        "    corpus1.append(res)\n",
        "\n",
        "corpus2 = []\n",
        "for abstract in corpus1 :\n",
        "    res = []\n",
        "    for sentence in abstract :\n",
        "        res.append(sentence.split(' '))\n",
        "    corpus2.append(res)\n",
        "corpus3 = []\n",
        "for i in range(0, acta_literaria.shape[0]):\n",
        "    res  = []\n",
        "    for element in acta_literaria['sentence'][i] :\n",
        "        #Remove punctuations\n",
        "        text = re.sub('[^a-zA-Z]', ' ', element)\n",
        "        \n",
        "        #Convert to lowercase\n",
        "        text = text.lower()\n",
        "        \n",
        "        #remove tags\n",
        "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "        \n",
        "        # remove special characters and digits\n",
        "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "        \n",
        "        ##Convert to list from string\n",
        "        text = text.split()\n",
        "        \n",
        "        text = \" \".join(text)\n",
        "        res.append(text)\n",
        "    corpus3.append(res)\n",
        "corpus4 = []\n",
        "\n",
        "for abstract in corpus3 :\n",
        "    res = []\n",
        "    for sentence in abstract :\n",
        "        res.append(sentence.split(' '))\n",
        "    corpus4.append(res)\n",
        "\n",
        "X = pd.DataFrame({\"corpus\" : corpus, \"corpus1\" : corpus1, \"corpus2\" : corpus2, \"corpus3\" : corpus3, \"corpus4\" : corpus4 })"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 37 entries, 1 to 40\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   abstract_en  37 non-null     object\n",
            " 1   keywords_en  37 non-null     object\n",
            "dtypes: object(2)\n",
            "memory usage: 888.0+ bytes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgNaDj5NekQY"
      },
      "source": [
        "# 3) Construction of the matrix of scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDm0w4CsgSNH"
      },
      "source": [
        "diverses foncitons à commenter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qesAKJJf7-c"
      },
      "source": [
        "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,1))\n",
        "cv_corpus=cv.fit_transform(X.corpus)\n",
        "\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(cv_corpus)\n",
        "# get feature names\n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "\n",
        "def extract_tf_idf_score(feature_names, items):\n",
        "      \"\"\"get the feature names and tf-idf score\"\"\"\n",
        "\n",
        "  \n",
        "      score_vals = []\n",
        "      feature_vals = []\n",
        "      \n",
        "      # word index and corresponding tf-idf score\n",
        "      for idx, score in items:\n",
        "          \n",
        "          #keep track of feature name and its corresponding score\n",
        "          score_vals.append(round(score, 3))\n",
        "          feature_vals.append(feature_names[idx])\n",
        "  \n",
        "      #create a tuples of feature,score\n",
        "      #results = zip(feature_vals,score_vals)\n",
        "      results= {}\n",
        "      for idx in range(len(feature_vals)):\n",
        "          results[feature_vals[idx]]=score_vals[idx]\n",
        "      \n",
        "      return results"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD8_WTTBgkiw"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3GMvk_QgtFD"
      },
      "source": [
        "Text rank "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxbnXIIKguBK"
      },
      "source": [
        "class TextRank4Keyword():\n",
        "    \"\"\"Extract keywords from text\"\"\"\n",
        "    \n",
        "    def __init__(self, abstract, d =  0.85, min_diff = 1e-5, steps = 10, node_weight = None, crit = 0.8):\n",
        "        self.d = d # damping coefficient, usually is .85\n",
        "        self.min_diff = min_diff # convergence threshold\n",
        "        self.steps = steps # iteration steps\n",
        "        self.node_weight = node_weight # save keywords and its weight\n",
        "        self.crit = crit\n",
        "        self.abstract = abstract\n",
        "        \n",
        "    def get_vocab(self):\n",
        "        \"\"\"Get all tokens\"\"\"\n",
        "        vocab = OrderedDict()\n",
        "        vocab1 = {}\n",
        "        i = 0\n",
        "        for sentence in self.abstract:\n",
        "            for word in self.abstract:\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = i\n",
        "                    i += 1\n",
        "                    vocab1[word] = 1\n",
        "                else : \n",
        "                    vocab1[word] += 1\n",
        "                    \n",
        "        return vocab, vocab1\n",
        "    \n",
        "    def get_token_pairs(self, window_size):\n",
        "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
        "        token_pairs = list()\n",
        "        for sentence in self.abstract:\n",
        "            for i, word in enumerate(sentence):\n",
        "                for j in range(i+1, i+window_size):\n",
        "                    if j >= len(sentence):\n",
        "                        break\n",
        "                    pair = (word, sentence[j])\n",
        "                    if pair not in token_pairs:\n",
        "                        token_pairs.append(pair)\n",
        "        return token_pairs\n",
        "        \n",
        "    def symmetrize(self, a):\n",
        "        return a + a.T - np.diag(a.diagonal())\n",
        "    \n",
        "    def get_matrix(self, vocab, token_pairs):\n",
        "        \"\"\"Get normalized matrix\"\"\"\n",
        "        # Build matrix\n",
        "        vocab_size = len(vocab)\n",
        "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
        "        for word1, word2 in token_pairs:\n",
        "            i, j = vocab[word1], vocab[word2]\n",
        "            g[i][j] = 1\n",
        "            \n",
        "        # Get Symmeric matrix\n",
        "        g = self.symmetrize(g)\n",
        "        \n",
        "        # Normalize matrix by column\n",
        "        norm = np.sum(g, axis=0)\n",
        "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
        "        \n",
        "        return g_norm\n",
        "\n",
        "    \n",
        "    def get_keywords(self, number=10):\n",
        "        \"\"\"Print top number keywords\"\"\"\n",
        "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
        "        for i, (key, value) in enumerate(node_weight.items()):\n",
        "            print(key + ' - ' + str(value))\n",
        "            if i > number:\n",
        "                break\n",
        "        \n",
        "    def get_scores(self, number=10):\n",
        "        \"\"\"Print top number keywords\"\"\"\n",
        "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
        "        keywords = {}\n",
        "        for i, (key, value) in enumerate(node_weight.items()):\n",
        "            print(key + ' - ' + str(value))\n",
        "            keywords[key] = value\n",
        "        return(keywords)\n",
        "            \n",
        "\n",
        "    def analyze(self,\n",
        "                window_size=4):\n",
        "        \"\"\"Main function to analyze text\"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "       \n",
        "        \n",
        "        # Build vocabulary\n",
        "        vocab, vocab1 = self.get_vocab(self.abstract)\n",
        "        \n",
        "        self.n_tot = 0\n",
        "        for key in vocab1.keys():\n",
        "            self.n_tot += vocab1[key]\n",
        "                \n",
        "        #Get rid of too occurent words\n",
        "        self.remove_list = []\n",
        "        for key in vocab1.keys():\n",
        "            if vocab1[key]/self.n_tot > self.crit :\n",
        "                vocab.pop(key)\n",
        "                self.remove_list.append(key)\n",
        "        \n",
        "        # Get token_pairs from windows\n",
        "        token_pairs = self.get_token_pairs(window_size, corpus2[0])\n",
        "        \n",
        "        # Get normalized matrix\n",
        "        g = self.get_matrix(vocab, token_pairs)\n",
        "        \n",
        "        # Initionlization for weight(pagerank value)\n",
        "        pr = np.array([1] * len(vocab))\n",
        "        \n",
        "        # Iteration\n",
        "        previous_pr = 0\n",
        "        for epoch in range(self.steps):\n",
        "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
        "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
        "                break\n",
        "            else:\n",
        "                previous_pr = sum(pr)\n",
        "\n",
        "        # Get weight for each node\n",
        "        node_weight = dict()\n",
        "        for word, index in vocab.items():\n",
        "            node_weight[word] = pr[index]\n",
        "        \n",
        "        self.node_weight = node_weight"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l25AE2BwWvF"
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wGuG20SwXP8"
      },
      "source": [
        "fonction pour textrank w2v"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB6JollCyfg0"
      },
      "source": [
        "def cosine_text_rank(w2v_mat) : \n",
        "  # similarity matrix, it is 30 times faster for sparse matrix\n",
        "  # replace this with A.dot(A.T).todense() for sparse representation\n",
        "  similarity = np.dot(w2v_mat, w2v_mat.T)\n",
        "\n",
        "  # squared magnitude of preference vectors (number of occurrences)\n",
        "  square_mag = np.diag(similarity)\n",
        "\n",
        "  # inverse squared magnitude\n",
        "  inv_square_mag = 1 / square_mag\n",
        "\n",
        "  # if it doesn't occur, set it's inverse magnitude to zero (instead of inf)\n",
        "  inv_square_mag[np.isinf(inv_square_mag)] = 0\n",
        "\n",
        "  # inverse of the magnitude\n",
        "  inv_mag = np.sqrt(inv_square_mag)\n",
        "\n",
        "  # cosine similarity (elementwise multiply by inverse magnitudes)\n",
        "  cosine = similarity * inv_mag\n",
        "  cosine = cosine.T * inv_mag\n",
        "  return(cosine)\n",
        "\n",
        "# pagerank powermethod\n",
        "def powerMethod(A, m, iter):\n",
        "    n = A.shape[1]\n",
        "    delta = m * (np.array([1] * n, dtype='float64') / n)\n",
        "    for i in range(iter):\n",
        "        x0 = np.dot((1 - m), np.dot(A, x0)) + delta\n",
        "    return x0"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6BDotShjqe4"
      },
      "source": [
        "Word 2 vec pretrained sur wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "l0eNmpTdTvSJ",
        "outputId": "419f7421-036d-46ae-e43f-07ee01b48f00"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-57b18cc2-7c78-402d-b743-ca68982e5338\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-57b18cc2-7c78-402d-b743-ca68982e5338\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-21dc3c638f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     result = _output.eval_js(\n\u001b[1;32m     71\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[0;32m---> 72\u001b[0;31m             output_id=output_id))\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'append'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0;31m# JS side uses a generator of promises to process all of the files- some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "K7fz7V7Wjp04",
        "outputId": "788df784-3d78-41b7-b802-dfba7f3ba32a"
      },
      "source": [
        "w2v = KeyedVectors.load_word2vec_format('C:/Users/Tom/Desktop/Cours/Ensae cours/nlp/projet/enwiki_20180420_100d.txt', binary=False)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-e4667e065769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'enwiki_20180420_100d.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'enwiki_20180420_100d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3ZuY9xxjpMv"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdD1qkt4fKjm"
      },
      "source": [
        "n_w2v_proj = 10\n",
        "n_w2v_tot = 100\n",
        "m = 0.15\n",
        "iter = 130\n",
        "data_final = pd.DataFrame()\n",
        "for i in range (n) : \n",
        "\n",
        "  #TF IDF SCORE\n",
        "\n",
        "  tf_idf_vector=tfidf_transformer.transform(cv.transform([X.corpus[i]]))\n",
        "\n",
        "  items=tuples = zip(tf_idf_vector.tocoo().col, tf_idf_vector.tocoo().data)\n",
        "  tf_idf = extract_tf_idf_score(feature_names,items)\n",
        "  \n",
        "  # tf_idf est un dico qui contient mots : tf_idf_score\n",
        "\n",
        "  #TextRank\n",
        "  t4r = TextRank4Keyword(corpus2[i])\n",
        "  t4r.analyze()\n",
        "  text_rank = t4r.get_scores()\n",
        "\n",
        "  # Word2vec entraîné sur wikipédia \n",
        "  w2v_list = []\n",
        "\n",
        "  for word in tf_idf.keys() :\n",
        "    w2v_list.append(w2v(word))\n",
        "\n",
        "  w2v_mat = np.array(w2v_list).reshape(len(tf_idf), n_w2v_tot)\n",
        "  pca = PCA(n_components = n_w2v_proj)\n",
        "\n",
        "  w2v_proj = pca.fit_transform(w2v_mat)\n",
        "  cosine = cosine_text_rank(w2v_mat)\n",
        "\n",
        "  x_0 = [1] * cosine.shape[1] \n",
        "\n",
        "  w2v_weights = powerMethod(cosine, x_0, m, iter)\n",
        "  w2v_weights = ( w2v_weights - w2v_weights.min() ) / ( w2v_weights.max() - w2v_weights.min() )\n",
        "\n",
        "  # RAKE \n",
        "  rake_fct = Rake(stopwords = stop_words) \n",
        "  rake_fct.extract_keywords_from_text(acta_literaria['abstract_en'][i])\n",
        "  degree = rake_fct.get_word_degrees()\n",
        "  freq = rake_fct.get_word_frequecy_distribution()\n",
        "  rake_score = {}\n",
        "\n",
        "  for word in tf_idf.keys() :\n",
        "    rake_score[word] = degree[word] / freq[word]\n",
        "\n",
        "  # YAKE! \n",
        "\n",
        "  max_ngram = 1 \n",
        "  d = 0.85\n",
        "  n_key = len(tf_idf)\n",
        "  lang = 'en'\n",
        "  yake_fct = yake.KeywordExtractor(lan = lang, n = max_ngram, dedupLim = d, top = n_key, features = None, stopwords = stop_word)\n",
        "  yake_kw_list = yake_fct.extract_keywords(acta_literaria['abstract_en'][i])\n",
        "  yake_kw = {yake_kw_list[i][0] : yake_kw_list[i][1] for i in range(len(yake_kw_list))}\n",
        "  data_score = {}\n",
        "  j = 0\n",
        "  for word in tf_idf.keys() : \n",
        "    data_score[word] = [tf_idf[word], yake_kw[word], rake_score[word], text_rank[word], w2v_weights[j] ] + w2v_proj[j, :].tolist() + [i]\n",
        "    j+=1\n",
        "\n",
        "  df_step = pd.DataFrame(data_score).T\n",
        "  data_final = pd.concat([data_final, df_step])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeQpVKsivOf1",
        "outputId": "b8402f4b-5a44-4cdc-e885-410a187cf4d5"
      },
      "source": [
        "import numpy as np\n",
        "np.array([[1,2],[1,2]]).tolist()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2], [1, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3g198_7vt_U"
      },
      "source": [
        "import pandas as pd\n",
        "a = pd.DataFrame({'a':[1,2],'b':[1,2]}).T\n",
        "b = pd.DataFrame({'a':[3,4],'b':[3,4]}).T"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "5pNWeAOzHw8Y",
        "outputId": "c846dbf8-de9f-409d-9bc2-697a56a12881"
      },
      "source": [
        "pd.concat([a,b])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>b</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>b</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1\n",
              "a  1  2\n",
              "b  1  2\n",
              "a  3  4\n",
              "b  3  4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "WeZ6mEDfO8i6",
        "outputId": "fed116b0-9eac-405a-b2e0-7fa2431c89ba"
      },
      "source": [
        "pd.concat([pd.DataFrame(),a])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>b</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1\n",
              "a  1  2\n",
              "b  1  2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGKWynjNPn34"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}